{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Deep Learning Models with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1667\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1068\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0244\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0420\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0543\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0890\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0800\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0919\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0132\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0186\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0142\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0309\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0629\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0224\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0450\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0195\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0274\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0291\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0148\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0118\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0884\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0129\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0467\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1015\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0130\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0150\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0334\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0103\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0008\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0194\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.41 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_batch = next(iter(test_loader))\n",
    "k = 7\n",
    "one_image = one_batch[0][k]\n",
    "one_label = one_batch[1][k]\n",
    "\n",
    "\n",
    "one_image_npy = one_image.squeeze().numpy()\n",
    "plt.imshow(one_image_npy, cmap='gray')\n",
    "\n",
    "image = one_image.reshape(1,28*28).to(device)\n",
    "label = one_label.to(device)\n",
    "print(one_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
